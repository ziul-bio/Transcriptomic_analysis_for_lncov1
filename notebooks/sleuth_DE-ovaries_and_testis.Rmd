---
title: "Diferencial Expression with Sleuth"
---

# Sleuth

Sleuth is a program for differential analysis of RNA-Seq data. It makes use of quantification uncertainty 
estimates obtained via kallisto for accurate differential analysis of isoforms or genes, allows testing in the
context of experiments with complex designs, and supports interactive exploratory data analysis via sleuth live.
The sleuth methods are described in:

H Pimentel, NL Bray, S Puente, P Melsted and Lior Pachter, Differential analysis of RNA-seq incorporating quantification uncertainty, Nature Methods (2017).

## Loading libraries 
```{r include=FALSE}
library(sleuth)
library(dplyr)
library(ggplot2)
library(ggrepel)
library(cowplot)
library(lemon)
library(openxlsx)
```


## Get help
```{r}
#vignette('intro', package = 'sleuth')
#help(package = 'sleuth')
#?sleuth_prep
```


# Sleuth for estimation of differential expression of transcripts

The workflow for Sleuth is similar to the workflow followed for DESeq2, even though, the models for estimating differential expression are very different.

Step 1: Creation of Sleuth object to provide metadata, estimated counts, and design formula for the analysis, in addition to a annotables database to switch between transcript IDs and associated gene names.

Step 2: Fit the sleuth model

Estimation of size (normalization) factors using the median of ratios method (similar to DESeq2)

Normalization of estimated counts using size factors (est. counts / size factors - similar to DESeq2)

Filtering of low abundance transcripts (< 5 est counts in more than 47% of the samples)

Normalization of technical variation estimates

Estimation of biological variance and shrinkage estimates (With small sample sizes, we will make very bad estimates of transcript-wise dispersion unless we share information across transcripts. Sleuth regularizes the biological variance estimate with shrinkage, similar to DESeq2, except uses a different statistical method (similar to Limma Voom).)

Parameter estimation and estimation of variance using the general linear model.

Identification of:

Coefficients: indicating overall expression strength
Beta values: estimates of fold changes

Step 3: Test for significant differences between conditions

After performing all analysis steps, we will explore the sample QC plots and plotting of results. In addition, we will use the html interface available through the sleuth package.


# Sleuth workflow

## Step 1: Create Sleuth object for analysis

Similar to DESeq2, we need to tell Sleuth where to find the metadata (specifying which samplegroups the samples belong to, and any other metadata we want included in the analysis), estimated counts (output from Salmon) and the design formula. In addition, we also need annotables to easily convert between transcript IDs and associated gene names.

To create this Sleuth object, we need to perform the following steps:

Create a dataframe containing metadata and locations of the estimated counts files:
- including any columns containing metadata to used in the analysis 
- a column named sample containing all of the sample names matching the names in the metadata file 
- a column named path containing the path to the abundance estimate files output from kallisto


## Creating a list of the paths to our transcript abundance files:

First, we create a simple vector containing the paths to the directories containing the transcript abundance 
estimates for each sample (folders containing the .quant files).
```{r}
base_dir <- "../results/denyse/kallisto/quant"
sample_id <- dir(file.path(base_dir))

# file.path() function gives the paths to each of the directories.
paths <- file.path(base_dir, sample_id)

paths
```


## Creating a metadata dataframe

Create metadata associated with the kallisto files as in the DESeq2 using the data.frame().
```{r}
# Sleuth requires a column entitled “sample” containing the sample names:
metadata <- data.frame(sample = sample_id,
                          condition = factor(c(rep("D", 3), rep("Q", 3), rep("W", 3)))
                          )

# Reordering group factor levels
metadata$condition <- factor(metadata$condition, levels = c("W", "D", "Q"))
```


## Naming the vector of directory paths with the corresponding sample names
```{r}
# Name the directory paths for the abundance files with their corresponding sample IDs
names(paths) <- sample_id

paths
```


## Combining the metadata with the paths 

Combining the metadata with the paths to the transcript abundance files to use as input for the Sleuth analysis.

Sleuth requires a column entitled “path” containing the paths to the estimated counts files stored in our sf_dirs:
```{r}
# Adding a column named 'path'
metadata$path <- paths

metadata
```



## Defining condition levels

NOTE: Sleuth will automatically use the first level (alphabetically by default) in the factor variable being 
tested to compare all other conditions against (in our metadata, this is ‘control’). 

If you want to use a different condition to be the base level, then you would need to use the relevel() function
to change the base level of the variable in step 1 above. 

For example, if we wanted the base level of condition to be “W”, we could use the following code:
```{r}
metadata$condition <- relevel(metadata$condition, ref = "W")
```

## Creating a variable containing the model design

With the metadata and location of the count estimates, we can input our design formula to determine the 
covariates and/or confounders that should be included in your experimental design model. 

Sleuth can be used to analyze multiple conditions from complex experimental designs. Within Sleuth, models are 
written similar to DESeq2.
```{r}
design <- ~ condition
```


# Step 2: Fit the sleuth model

Fit the transcript abundance data to the Sleuth model Using the sleuth_prep() function, the counts are normalized 
and filtered, then merged with the metadata. In addition, the bootstraps for each transcript are summarized. 

This function can take a bit of time, but there is an option (ncores) to split across multiple processors.
```{r}
# Create sleuth object for analysis 

basic_filter <- function (row, min_reads = 5, min_prop = 0.47) {
  mean(row >= min_reads) >= min_prop
  }


so <- sleuth_prep(metadata, 
                  full_model = design, 
                  #target_mapping = t2g,
                  num_cores = 4L,
                  read_bootstrap_tpm = TRUE,
                  extra_bootstrap_summary = TRUE,
                  transform_fun_counts = function(x) log2(x + 0.5),
                  filter_fun = basic_filter
                  )
```
NOTE: By default the transformation of counts is natural log, which would make the output fold changes somewhat 
more difficult to interpret. By specifying the transform_fun_counts to be log2(x + 0.5) we are ensuring our 
output fold changes are log2.

offset = 0.5 is need to to prevent taking the log of 0.

## Fitting the sleuth model 

sleuth performs shrinkage of variance, parameter estimation and estimation of variance using the general linear model:
```{r}
so <- sleuth_fit(so)
```


## The statistics "under the hood"

sleuth looks at the overall variance of a gene, and then first attempts to estimate what part of that variance
is from the "technical variance" introduced by kallisto (in bootstrap), and then compares the variance within
a group (Biological repricates) to the variance between groups (treatment groups).


## Check which models have been fit and which coefficients can be tested

Ensure the design model and coefficients are correct for your analysis. 

The level not shown is the base level.
```{r}
models(so)
```


## Step 3: Test significant differences between conditions using the Wald test

At this step in the workflow, we need to specify which level we want to compare against the base level 
(use the name given for the coefficients from models(so)):

# Wald test for differential expression of isoforms
```{r}
DE_D <- sleuth_wt(so, which_beta = 'conditionD')
DE_Q <- sleuth_wt(so, which_beta = 'conditionQ')



# Get results
sleuth_results_D <- sleuth_results(DE_D, test = 'conditionD', show_all = TRUE)

sleuth_results_Q <- sleuth_results(DE_Q, test = 'conditionQ', show_all = TRUE)

```
NOTE: The output represents the results from the differential expression testing with the following columns:

* target_id: the Ensembl transcript ID
* pval: the Wald test FDR adjusted pvalue using Benjamini-Hochberg
* qval: the p-value adjusted for multiple test correction
* b: beta value, which is the log2 fold changes between conditions (These are log2 b/c we specified log2 
transformation in the sleuth_prep() step. By default, these would have been natural log fold changes).
* se_b: standard error of the beta value
* mean_obs: the mean expression (log2) of the transcript across all samples
* var_obs: the biological variance of the expression
* tech_var: the technical variance of expression (derived from the bootstraps)
* sigma_sq: raw estimator of the variance once the technical variance has been removed
* smooth_sigma_sq: the smooth regression fit for the shrinkage estimation
* final_sigma_sq: max(sigma_sq, smooth_sigma_sq). this is the one used for covariance estimation of beta 
(in addition to tech_var)
* ens_gene: associated Ensembl gene ID
* ext_gene: associated gene symbol

Salving results table
```{r}
#write.xlsx(sleuth_results_D, 'results/sleuth_results_D.xlsx', col_names = TRUE)

#write.xlsx(sleuth_results_Q, 'results/sleuth_results_Q.xlsx', col_names = TRUE)

```


## Acesse o Shiny dos resultados (precisa de tela gráfica):
```{r}
#sleuth_live(so)
```



# Results and visualization


Transcrits of interest: "XM_001120691.5", "XM_393605.7", "XM_624635.6", "lncov1"
```{r}
txI <- c("XM_001120691.5", "XM_624635.6", "lncov1", "XM_006569892.2") 
```


## 1. Get Differencial expressed transcripts results table
```{r}
sleuth_sig_D <- dplyr::filter(sleuth_results_D, pval <= 0.05)
sleuth_sig_Q <- dplyr::filter(sleuth_results_Q, pval <= 0.05)

head(sleuth_sig_Q)
```

# 2. Volcano plot

volcano plot, Plots of beta value (regression) versus log of significance p-values.

## Volcano plot Drone vs Worker
```{r, fig.height=5, fig.width=9}
q <- as.data.frame(sleuth_results_D)
q <- na.omit(q)
q$significant <- ifelse(q$qval<0.05, "True", "False")
q[which(abs(q$b)<1),'significant'] <- "False"
q <- q[order(q$qval),]


legen <- q[q$target_id %in% txI, ]
legen$target_id <- recode(legen$target_id, XM_001120691.5 = "LOC726407", 
                          #XM_393605.7 = "Gapdh" ,
                          XM_624635.6 = "Tudor-SN",
                          XM_006569892.2 = "Ef1α"  )

volcano = ggplot(q, aes(b, -log10(qval))) + 
  geom_point(aes(col=significant)) +
  scale_color_manual(values=c("red", "gray"))


volcano + geom_label_repel(data= legen, aes(label= legen$target_id), size=4, 
  box.padding = unit(2, "lines"), point.padding = unit(5, "points")) +
  labs(title= "Volcano plot of Diferencial Expressed Transcipts",
       subtitle = "Comparison of Drone vs Worker",
       x= expression(paste("Log"[2], " fold change")), 
       y= expression(paste("q-value (-log" [10], ")")),
       color="Differentially Expressed") +
  theme_bw() + coord_cartesian(clip = "off")
ggsave(file="results/volcano1_Drone_x_worker_v02.jpeg", height=5, width=9, dpi=300)
```


## Volcano plot Normal Queen vs Worker
```{r, , fig.height=5, fig.width=9}
q <- as.data.frame(sleuth_results_Q)
q <- na.omit(q)
q$significant <- ifelse(q$qval<0.05, "True", "False")
q[which(abs(q$b)<1),'significant'] <- "False"
q <- q[order(q$qval),]


legen <- q[q$target_id %in% txI, ]
legen$target_id <- recode(legen$target_id, XM_001120691.5 = "LOC726407", 
                          #XM_393605.7 = "Gapdh" ,
                          XM_624635.6 = "Tudor-SN",
                          XM_006569892.2 = "Ef1α"  )

volcano = ggplot(q, aes(b, -log10(qval))) + 
  geom_point(aes(col=significant)) +
  scale_color_manual(values=c("red", "gray"))


volcano + geom_label_repel(data= legen, aes(label= legen$target_id), size=4, 
  box.padding = unit(2, "lines"), point.padding = unit(5, "points")) + ylim(0,1000) +
  labs(title= "Volcano plot of Diferencial Expressed Transcipts",
       subtitle = "Comparison of Queen vs Worker",
       x= expression(paste("Log"[2], " fold change")), 
       y= expression(paste("q-value (-log"[10], ")")),
       color="Differentially Expressed") +
  theme_bw() + coord_cartesian(clip = "off")
ggsave(file="results/volcano2_Queen_x_worker_v02.jpeg", height=5, width=9, dpi=300)
```


## 3. PCA
```{r, fig.height=5, fig.width=9}
#png("pca-transcritos.png",height = 8, width = 10, units = 'in',res=600)

plot_pca(so, pc_x = 1L, pc_y = 2L, use_filtered = TRUE,
  units = "est_counts", text_labels = F, color_by = "condition",
  point_size = 4) +
  theme_bw() + # remove default ggplot2 theme
  ggtitle(label = "Principal Component Analysis (PCA)", 
          subtitle = "Variance among worker, queenless-worker and queen")
```


## 4. Density
```{r, fig.height=5, fig.width=9}
plot_group_density(so, use_filtered = TRUE, units = "est_counts",
                   trans = "log", grouping = "condition", offset = 1)  
```


## 5. Boxplots

Transcript lncov1
```{r, fig.height=6, fig.width=10}
names_facet <- c(W = "Queenright workers", D = "Drone", Q = "Queen")

plot1 <- plot_bootstrap(so, "lncov1", units = "est_counts", color_by = "condition") +

ggtitle("lncov1") + theme_bw(11) + 
 theme(legend.position="none", plot.title = element_text(size = 14, face="italic")) +
  ylab("Estimated counts") + xlab("Samples") +  
  facet_rep_wrap(~condition, labeller = as_labeller(names_facet), scales='free_x', repeat.tick.labels = "All", strip.position="bottom") +
  scale_fill_manual(values=c("#7f7efd","#DCDCDC","#74C476")) +
  ylim(7.5, 11.5)

plot1
#ggsave(file="results/boxplot1_lncov1.jpeg", height=8, width=11, dpi=300)
```



XM_006569892.2 = ef1alfa
```{r, fig.height=6, fig.width=10}
plot3 <- plot_bootstrap(so, "XM_006569892.2", units = "est_counts", color_by = "condition") +

ggtitle("Ef1α") + theme_bw(11) + 
  theme(legend.position="none", plot.title = element_text(size = 14, face="italic")) +
  ylab("Estimated counts") + xlab("Samples") +  
  facet_rep_wrap(~condition, labeller = as_labeller(names_facet), scales='free_x', repeat.tick.labels = "All", strip.position="bottom") +
  facet_rep_wrap(~condition, labeller = as_labeller(names_facet), scales='free_x', 
                 repeat.tick.labels = "All", strip.position="bottom", ncol = 5) + 
  scale_fill_manual(values=c("#7f7efd","#DCDCDC","#74C476"))+
  ylim(7.5, 11.5)


plot3
#ggsave(file="results/boxplot3_ef1alfa_v02.jpeg", height=8, width=11, dpi=300)
```


Distribuition of est_counts of transcripts of interest:
```{r, fig.height=8, fig.width=9}
plot_grid(plot1, #+ theme(legend.position="right"),  
          #plot2 + theme(legend.position="right"), 
          plot3,# + theme(legend.position="right"),
          #plot4 + theme(legend.position="right"),
          #plot4 + theme(legend.position="none") + ylab(NULL),
          labels = c("A", "B"),
          ncol = 1, nrow = 2)

ggsave(file="../results/denyse/figures/boxplot_ovaries_and_testis_lncov1_elf1a.jpeg", height=8, width=9, dpi=600)
```


## 6. Heatmaps

we can perform an expression heatmap for select transcripts, transcripts of interest:
```{r, fig.height=5, fig.width=9}
# Filtando a matriz pelos transcritos de interesse
mat <- sleuth_results_Q[sleuth_results_Q$target_id %in% txI, ]
  
plot_transcript_heatmap(DE_Q, units = "est_counts", transcripts = mat$target_id)
```

## Referências



